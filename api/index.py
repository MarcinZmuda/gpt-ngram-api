from flask import Flask, request, jsonify
from collections import Counter
import spacy

# --- Importowanie lokalnych modu≈Ç√≥w ---
# Upewnij siƒô, ≈ºe pliki .py sƒÖ w tym samym katalogu (lub pakiecie)
# i ≈ºe plik generate_compliance_report.py zosta≈Ç zaktualizowany
# do wersji stanowej (v4.1), kt√≥rƒÖ poda≈Çem wcze≈õniej.
try:
    from .synthesize_topics import synthesize_topics
    from .generate_compliance_report import generate_compliance_report
except ImportError:
    # Fallback dla uruchomienia bezpo≈õrednio (np. python index.py)
    print("Uwaga: Uruchamianie w trybie fallback import (bez .)")
    from synthesize_topics import synthesize_topics
    from generate_compliance_report import generate_compliance_report


# ======================================================
# üåç Inicjalizacja aplikacji Flask
# ======================================================
app = Flask(__name__)

# Za≈Çaduj model jƒôzyka polskiego (spaCy)
try:
    nlp = spacy.load("pl_core_news_sm")
except OSError:
    print("Model pl_core_news_sm nie znaleziony. Pr√≥ba pobrania...")
    from spacy.cli import download
    download("pl_core_news_sm")
    nlp = spacy.load("pl_core_news_sm")

# ======================================================
# üß© 1Ô∏è‚É£ Endpoint: analiza n-gram√≥w i encji (Bez zmian)
# ======================================================
@app.route("/api/ngram_entity_analysis", methods=["POST"])
def perform_ngram_analysis():
    """
    Analizuje tekst pod kƒÖtem encji (entities) i n-gram√≥w (2-, 3-, 4-gram√≥w)
    oraz nadaje im priorytety na podstawie kontekstu SERP.
    """
    data = request.get_json()
    text = data.get("text", "")
    main_keyword = data.get("main_keyword", "")
    serp_context = data.get("serp_context", {})  # optional

    if not text.strip():
        return jsonify({"error": "Brak tekstu do analizy"}), 400

    doc = nlp(text)

    # --- Wykrywanie encji (entities) ---
    entities = list({ent.text for ent in doc.ents if len(ent.text) > 2})

    # --- Tokenizacja (oryginalne s≈Çowa + stop-words) ---
    tokens = [t.text.lower() for t in doc if t.is_alpha or t.is_stop]

    # --- Tworzenie n-gram√≥w (2‚Äì4) ---
    ngram_results = {}
    for n in range(2, 5):
        grams = Counter([" ".join(tokens[i:i + n]) for i in range(len(tokens) - n + 1)])
        ngram_results[f"{n}gram"] = [{"ngram": g, "count": c} for g, c in grams.most_common(25)]

    # --- Priorytetyzacja (je≈õli kontekst SERP jest dostƒôpny) ---
    paa = " ".join(serp_context.get("people_also_ask", []))
    related = " ".join(serp_context.get("related_searches", []))
    snippets = " ".join(serp_context.get("featured_snippets", []))
    all_context = f"{paa} {related} {snippets}".lower()

    for key in ngram_results:
        for item in ngram_results[key]:
            phrase = item["ngram"]
            priority = 1
            if phrase in all_context:
                priority += 2
            if main_keyword and main_keyword.lower() in phrase:
                priority += 1
            item["priority"] = priority
        # Sortowanie po priorytecie
        ngram_results[key] = sorted(
            ngram_results[key],
            key=lambda x: (x["priority"], x["count"]),
            reverse=True
        )

    # --- Finalna odpowied≈∫ ---
    return jsonify({
        "entities": entities[:15],
        "ngrams": ngram_results,
        "main_keyword": main_keyword,
        "summary": {
            "total_entities": len(entities),
            "text_length": len(text),
            "context_used": bool(serp_context)
        }
    })


# ======================================================
# üß© 2Ô∏è‚É£ Endpoint: synteza temat√≥w (Bez zmian)
# ======================================================
@app.route("/api/synthesize_topics", methods=["POST"])
def perform_synthesize_topics():
    """
    Tworzy syntetyczne tematy i powiƒÖzania semantyczne na podstawie
    n-gram√≥w i nag≈Ç√≥wk√≥w (H2) zanalizowanych wcze≈õniej.
    """
    data = request.get_json()
    ngrams = data.get("ngrams", [])
    headings = data.get("headings", [])

    # Zak≈ÇadajƒÖc, ≈ºe synthesize_topics przyjmuje ngrams i headings
    result = synthesize_topics(ngrams, headings)
    return jsonify(result)


# ======================================================
# üß© 3Ô∏è‚É£ Endpoint: raport jako≈õci tre≈õci (WERSJA STANOWA v4.1)
# ======================================================
@app.route("/api/generate_compliance_report", methods=["POST"])
def perform_generate_compliance_report():
    """
    Analizuje zgodno≈õƒá tre≈õci z za≈Ço≈ºonymi s≈Çowami kluczowymi (STANOWO).
    Sprawdza u≈ºycie w batchu i zwraca nowy stan.
    """
    data = request.get_json()
    text = data.get("text", "") # Tekst TYLKO z bie≈ºƒÖcego batcha
    
    # Oczekujemy klucza 'keyword_state' z master_api.py (zgodnego z v4.1)
    keyword_state_input = data.get("keyword_state") 

    # Fallback dla kompatybilno≈õci (gdyby master_api wys≈Ça≈Ç stary klucz 'keywords')
    if not keyword_state_input:
        keyword_state_input = data.get("keywords")
        
    if not keyword_state_input:
         return jsonify({"error": "Brak 'keyword_state' (lub 'keywords') w payloadzie"}), 400

    # Wywo≈Çanie nowej, stanowej funkcji
    result = generate_compliance_report(text, keyword_state_input) 
    return jsonify(result)


# ======================================================
# üß© 4Ô∏è‚É£ Endpoint: testowy root (Bez zmian)
# ======================================================
@app.route("/", methods=["GET"])
def root():
    return jsonify({"message": "GPT N-Gram & Entity API (Stateful v4.1) dzia≈Ça poprawnie."})


# ======================================================
# ü©∫ 5Ô∏è‚É£ Health Check (Bez zmian)
# ======================================================
@app.route("/api/health", methods=["GET"])
def health_check():
    return jsonify({
        "status": "‚úÖ API dzia≈Ça poprawnie",
        "version": "v4.1.0-stateful", # Zmieniona wersja dla jasno≈õci
        "message": "gpt-ngram-api online"
    }), 200


# ======================================================
# üöÄ Uruchomienie lokalne
# ======================================================
if __name__ == "__main__":
    # U≈ºywamy portu 5000, zgodnie z Twoim render.yaml
    app.run(host="0.0.0.0", port=5000, debug=True)
