"""
===============================================================================
ğŸ§  TOPICAL ENTITY EXTRACTOR v1.0 â€” Encje pojÄ™ciowe (Concept/Semantic Entities)
===============================================================================
WyciÄ…ga topical entities z tekstÃ³w konkurencji â€” pojÄ™cia, koncepty, frazy
tematyczne, ktÃ³re NIE SÄ„ named entities (osoby/firmy/miejsca), ale sÄ…
kluczowe dla pokrycia tematu i Google Knowledge Graph.

Metody ekstrakcji:
1. spaCy noun_chunks â†’ frazy rzeczownikowe ("bezpieczny transport", "dokumenty firmowe")
2. Frequency Ã— Distribution scoring â†’ waÅ¼niejsze = w wielu ÅºrÃ³dÅ‚ach
3. Lemmatyzacja â†’ grupuje odmiany ("dokumentÃ³w"/"dokumenty"/"dokumentami")
4. Garbage filtering â†’ eliminuje CSS/JS/HTML artefakty

Integracja: entity_extractor.py â†’ perform_entity_seo_analysis()

Autor: BRAJEN Team
Data: 2025
===============================================================================
"""

import re
import math
from collections import Counter, defaultdict
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, field


# ================================================================
# ğŸš« STOP WORDS & FILTERS
# ================================================================

# Polskie stop words (rozszerzone)
_STOP_WORDS_PL = {
    "i", "w", "na", "z", "do", "Å¼e", "siÄ™", "nie", "to", "jest", "za", "po",
    "od", "o", "jak", "ale", "co", "ten", "tym", "byÄ‡", "moÅ¼e", "juÅ¼", "tak",
    "gdy", "lub", "czy", "tego", "tej", "sÄ…", "dla", "ich", "przez", "jako",
    "te", "ze", "tych", "byÅ‚o", "ma", "przy", "tym", "ktÃ³re", "ktÃ³ry", "ktÃ³ra",
    "ktÃ³rych", "jego", "jej", "tego", "takÅ¼e", "wiÄ™c", "tylko", "teÅ¼", "sobie",
    "bardzo", "jeszcze", "wszystko", "przed", "miÄ™dzy", "pod", "nad", "bez",
    "oraz", "gdzie", "kiedy", "ile", "jeÅ›li", "jaki", "jaka", "jakie",
    "tutaj", "tam", "teraz", "potem", "zawsze", "nigdy", "kaÅ¼dy", "kaÅ¼da",
    "mieÄ‡", "mÃ³c", "musieÄ‡", "chcieÄ‡", "wiedzieÄ‡", "duÅ¼o", "maÅ‚o",
    "ten", "ta", "to", "ci", "te", "tÄ™", "tÄ…", "tych", "tym", "tymi",
    "mÃ³j", "twÃ³j", "swÃ³j", "nasz", "wasz", "moje", "twoje", "nasze",
    "jeden", "dwa", "trzy", "cztery", "piÄ™Ä‡", "kilka", "wiele",
    "co", "kto", "czego", "czym", "komu", "kogo", "kim",
    "strona", "link", "kliknij", "czytaj", "wiÄ™cej", "dalej",
    "udostÄ™pnij", "komentarz", "odpowiedz", "przeczytaj",
}

# ================================================================
# ğŸš« WEB GARBAGE FILTER â€” import or fallback
# ================================================================

try:
    try:
        from .web_garbage_filter import is_entity_garbage as _is_web_garbage, CSS_ENTITY_BLACKLIST
    except ImportError:
        from web_garbage_filter import is_entity_garbage as _is_web_garbage, CSS_ENTITY_BLACKLIST
    WEB_FILTER_AVAILABLE = True
    print(f"[TOPICAL] âœ… Web garbage filter loaded ({len(CSS_ENTITY_BLACKLIST)} entries)")
except ImportError:
    WEB_FILTER_AVAILABLE = False
    CSS_ENTITY_BLACKLIST = set()
    print("[TOPICAL] âš ï¸ web_garbage_filter not found, using built-in patterns")


# Wzorce garbage (CSS/JS/HTML) w noun chunks â€” used as fallback
_GARBAGE_CHUNK_PATTERNS = re.compile(
    r'[{};@#\[\]<>=\\|]|'       # Specjalne znaki CSS/JS
    r'\.ast[-_]|\.wp[-_]|'       # WordPress/Astra CSS
    r'webkit|moz-|flex-|'        # CSS vendor prefixes
    r'font-|border-|padding-|'   # CSS properties
    r'display:|color:|margin:|'  # CSS declarations
    r'kevlar_|ytcfg|ytplayer|'   # YouTube JS
    r'px$|em$|rem$|vh$|vw$|'     # CSS units
    r'var\(|calc\(|rgb\(|'       # CSS functions
    r'none|inherit|auto|'        # CSS values
    r'data-|aria-|role=',        # HTML attributes
    re.IGNORECASE
)

# Minimum proporcja liter w chunku (odfiltruj "123px", "50%" etc.)
_MIN_ALPHA_RATIO = 0.6


# ================================================================
# ğŸ“¦ STRUKTURY DANYCH
# ================================================================

@dataclass
class TopicalEntity:
    """Encja pojÄ™ciowa / topical entity."""
    text: str                    # Tekst encji (zlematyzowana forma bazowa)
    display_text: str            # NajczÄ™stsza forma powierzchniowa
    type: str = "CONCEPT"        # CONCEPT (1-2 sÅ‚owa) lub TOPICAL (2-4 sÅ‚owa)
    frequency: int = 1           # ÅÄ…czna czÄ™stoÅ›Ä‡ we wszystkich ÅºrÃ³dÅ‚ach
    sources_count: int = 1       # W ilu ÅºrÃ³dÅ‚ach wystÄ™puje
    importance: float = 0.5      # Score 0-1
    contexts: List[str] = field(default_factory=list)
    variants: List[str] = field(default_factory=list)  # Odmiany
    freq_per_source: List[int] = field(default_factory=list)  # v51: per-source frequency
    
    def to_dict(self) -> Dict:
        # v51: Compute Surfer-style frequency ranges
        non_zero = sorted([c for c in self.freq_per_source if c > 0]) if self.freq_per_source else []
        if non_zero:
            freq_min = non_zero[0]
            freq_max = non_zero[-1]
            mid = len(non_zero) // 2
            freq_median = non_zero[mid] if len(non_zero) % 2 == 1 else (non_zero[mid-1] + non_zero[mid]) // 2
        else:
            freq_min = freq_median = freq_max = 0
        
        return {
            "text": self.display_text,
            "type": self.type,
            "frequency": self.frequency,
            "sources_count": self.sources_count,
            "importance": round(self.importance, 3),
            "sample_context": self.contexts[0] if self.contexts else "",
            "variants": self.variants[:5],
            "freq_per_source": self.freq_per_source,
            "freq_min": freq_min,
            "freq_median": freq_median,
            "freq_max": freq_max,
        }


# ================================================================
# ğŸ”§ HELPER FUNCTIONS
# ================================================================

def _is_chunk_garbage(text: str) -> bool:
    """Sprawdza czy noun chunk to CSS/JS/HTML garbage."""
    if not text or len(text) < 2:
        return True
    
    # Use comprehensive filter if available
    if WEB_FILTER_AVAILABLE and _is_web_garbage(text):
        return True
    
    # Fallback: Garbage pattern check
    if _GARBAGE_CHUNK_PATTERNS.search(text):
        return True
    
    # Alpha ratio check
    alpha_count = sum(1 for c in text if c.isalpha())
    if len(text) > 0 and alpha_count / len(text) < _MIN_ALPHA_RATIO:
        return True
    
    # Too short or too long
    if len(text) < 3 or len(text) > 80:
        return True
    
    return False


def _is_only_stopwords(words: List[str]) -> bool:
    """Sprawdza czy fraza skÅ‚ada siÄ™ tylko ze stop words."""
    meaningful = [w for w in words if w.lower() not in _STOP_WORDS_PL and len(w) > 2]
    return len(meaningful) == 0


def _normalize_chunk(text: str) -> str:
    """Normalizuje noun chunk do porÃ³wnywania."""
    # Lowercase, strip, remove extra whitespace
    text = re.sub(r'\s+', ' ', text.lower().strip())
    # Remove leading/trailing punctuation
    text = text.strip('.,;:!?-â€“â€”()[]"\'')
    return text


def _pos_noun_chunks(doc):
    """
    Fallback for languages without noun_chunks (e.g. Polish).
    Builds noun-phrase spans from POS tags: sequences of ADJ/NOUN/PROPN
    ending in NOUN or PROPN, length 2-5 tokens.
    Returns list of spaCy Span objects (same interface as doc.noun_chunks).
    """
    _NP_POS = {"NOUN", "PROPN", "ADJ"}
    spans = []
    start = None
    for i, token in enumerate(doc):
        if token.pos_ in _NP_POS and not token.is_stop:
            if start is None:
                start = i
        else:
            if start is not None:
                end = i
                length = end - start
                if 2 <= length <= 5 and doc[end - 1].pos_ in {"NOUN", "PROPN"}:
                    spans.append(doc[start:end])
                start = None
    # handle tail
    if start is not None:
        end = len(doc)
        length = end - start
        if 2 <= length <= 5 and doc[end - 1].pos_ in {"NOUN", "PROPN"}:
            spans.append(doc[start:end])
    return spans


def _get_lemma_key(doc_chunk) -> str:
    """
    Tworzy klucz lemmatyzacyjny z noun chunka.
    UÅ¼ywa lemmatÃ³w spaCy do grupowania odmian.
    """
    lemmas = []
    for token in doc_chunk:
        if token.is_stop or token.is_punct:
            continue
        lemma = token.lemma_.lower()
        if len(lemma) > 2:
            lemmas.append(lemma)
    
    return " ".join(sorted(lemmas))  # Sorted dla niezaleÅ¼noÅ›ci od kolejnoÅ›ci


def _get_context(text: str, phrase: str, window: int = 60) -> str:
    """WyciÄ…ga kontekst wokÃ³Å‚ frazy."""
    idx = text.lower().find(phrase.lower())
    if idx < 0:
        return ""
    start = max(0, idx - window)
    end = min(len(text), idx + len(phrase) + window)
    ctx = text[start:end].strip()
    if start > 0:
        ctx = "..." + ctx
    if end < len(text):
        ctx = ctx + "..."
    return ctx


# ================================================================
# ğŸ§  MAIN EXTRACTION
# ================================================================

def extract_topical_entities(
    nlp,
    texts: List[str],
    urls: List[str] = None,
    main_keyword: str = "",
    max_entities: int = 30,
    min_frequency: int = 2,
    min_sources: int = 1,
) -> List[TopicalEntity]:
    """
    WyciÄ…ga topical/concept entities z tekstÃ³w konkurencji.
    
    UÅ¼ywa spaCy noun_chunks zamiast NER â€” wyciÄ…ga frazy rzeczownikowe
    jak "bezpieczny transport", "dokumenty firmowe", "karton do przeprowadzki".
    
    Args:
        nlp: ZaÅ‚adowany model spaCy
        texts: Lista tekstÃ³w (z content_extractor lub SERP scraper)
        urls: Lista URL-i ÅºrÃ³deÅ‚
        main_keyword: Fraza gÅ‚Ã³wna (do boostowania relevance)
        max_entities: Max encji do zwrÃ³cenia
        min_frequency: Min czÄ™stoÅ›Ä‡ (suma ze wszystkich ÅºrÃ³deÅ‚)
        min_sources: Min ile ÅºrÃ³deÅ‚ musi zawieraÄ‡ frazÄ™
    
    Returns:
        Lista TopicalEntity posortowana po importance
    """
    if not texts:
        return []
    
    urls = urls or [f"source_{i}" for i in range(len(texts))]
    total_sources = len(texts)
    
    # Struktura do agregacji
    # klucz = lemma_key, wartoÅ›ci = dane
    chunk_data = defaultdict(lambda: {
        "frequency": 0,
        "sources": set(),
        "surface_forms": Counter(),  # Zlicza formy powierzchniowe
        "contexts": [],
        "word_count": 0,
        "freq_per_source": Counter(),  # v51: per-source frequency
    })
    
    # Keyword lemmas (do relevance boosting)
    keyword_words = set()
    if main_keyword:
        keyword_words = {w.lower() for w in main_keyword.split() 
                        if w.lower() not in _STOP_WORDS_PL and len(w) > 2}
    
    print(f"[TOPICAL] ğŸ” Extracting concept entities from {len(texts)} sources")
    
    for idx, text in enumerate(texts):
        if not text or len(text) < 100:
            continue
        
        # Limit tekstu dla wydajnoÅ›ci
        text_sample = text[:50000]
        
        try:
            doc = nlp(text_sample)

            # Polish spaCy models don't support noun_chunks,
            # so we build them from POS tags (NOUN/PROPN/ADJ sequences)
            try:
                chunks = list(doc.noun_chunks)
            except NotImplementedError:
                chunks = _pos_noun_chunks(doc)

            for chunk in chunks:
                chunk_text = chunk.text.strip()
                
                # --- FILTRACJA ---
                
                # Skip garbage
                if _is_chunk_garbage(chunk_text):
                    continue
                
                # Normalizuj
                normalized = _normalize_chunk(chunk_text)
                if not normalized or len(normalized) < 3:
                    continue
                
                # Podziel na sÅ‚owa
                words = normalized.split()
                
                # Skip single stopwords lub pure numbers
                if _is_only_stopwords(words):
                    continue
                
                # Skip za dÅ‚ugie frazy (>5 sÅ‚Ã³w = prawdopodobnie zdanie)
                if len(words) > 5:
                    continue
                
                # Skip jeÅ›li pierwsze sÅ‚owo to przyimek/zaimek (np. "w domu", "to jest")
                if words[0] in _STOP_WORDS_PL and len(words) <= 2:
                    continue
                
                # --- GRUPOWANIE po lematach ---
                lemma_key = _get_lemma_key(chunk)
                if not lemma_key or len(lemma_key) < 3:
                    continue
                
                data = chunk_data[lemma_key]
                data["frequency"] += 1
                data["sources"].add(urls[idx])
                data["surface_forms"][normalized] += 1
                data["word_count"] = max(data["word_count"], len(words))
                data["freq_per_source"][idx] += 1  # v51: per-source tracking
                
                # Kontekst (max 3 per entity)
                if len(data["contexts"]) < 3:
                    ctx = _get_context(text_sample, chunk_text)
                    if ctx and ctx not in data["contexts"]:
                        data["contexts"].append(ctx)
        
        except Exception as e:
            print(f"[TOPICAL] âš ï¸ Error processing source {idx}: {e}")
            continue
    
    # --- SCORING & RANKING ---
    entities = []
    
    for lemma_key, data in chunk_data.items():
        freq = data["frequency"]
        sources_count = len(data["sources"])
        
        # Minimum thresholds
        if freq < min_frequency:
            continue
        if sources_count < min_sources:
            continue
        
        # NajczÄ™stsza forma powierzchniowa = display text
        # v52.1: Spellcheck â€” preferuj formy bez oczywistych literÃ³wek.
        # LiterÃ³wka = sÅ‚owo z sekwencjÄ… 3+ spÃ³Å‚gÅ‚osek BEZ samogÅ‚oski (np. "uchwtÃ³w")
        # lub krÃ³tkie sÅ‚owo z brakujÄ…cymi literami w Å›rodku.
        def _has_typo(word: str) -> bool:
            """Heurystyczna detekcja literÃ³wki w polskim sÅ‚owie.

            v52.1: ReguÅ‚a: 4+ spÃ³Å‚gÅ‚osek z rzÄ™du po pierwszej samogÅ‚osce = literÃ³wka.
            Polskie grupy na POCZÄ„TKU sÅ‚owa (strz, prz, chr, szcz) sÄ… dozwolone.
            W Å›rodku sÅ‚owa sekwencja 4+ spÃ³Å‚gÅ‚osek = brakujÄ…ce litery.
            PrzykÅ‚ad: 'uchwtÃ³w' â†’ po 'u' pojawia siÄ™ 'chwt' (4 spÃ³Å‚gÅ‚oski) = literÃ³wka.
            """
            vowels = set("aÄ…eÄ™ioÃ³uy")
            w = word.lower()
            i = 0
            # PomiÅ„ poczÄ…tkowÄ… grupÄ™ spÃ³Å‚gÅ‚osek (dozwolone w polskim: strz, prz, chr itp.)
            while i < len(w) and w[i].isalpha() and w[i] not in vowels:
                i += 1
            # Skanuj resztÄ™ sÅ‚owa â€” mid-word 4+ spÃ³Å‚gÅ‚oski = literÃ³wka
            cluster = 0
            while i < len(w):
                ch = w[i]
                if not ch.isalpha():
                    cluster = 0
                elif ch in vowels:
                    cluster = 0
                else:
                    cluster += 1
                    if cluster >= 4:
                        return True
                i += 1
            return False

        def _best_surface_form(surface_forms_counter):
            """Wybiera najczÄ™stszÄ… formÄ™, ale pomija ewidentne literÃ³wki."""
            candidates = surface_forms_counter.most_common()
            for form, count in candidates:
                words_in_form = form.split()
                if not any(_has_typo(w) for w in words_in_form):
                    return form
            # Fallback: zwrÃ³Ä‡ najczÄ™stszÄ… mimo potencjalnej literÃ³wki
            return candidates[0][0]

        most_common_form = _best_surface_form(data["surface_forms"])

        # Wszystkie warianty (bez literÃ³wek na pierwszym miejscu)
        variants = [form for form, _ in data["surface_forms"].most_common(5)]
        
        # Typ: CONCEPT (1-2 sÅ‚owa) lub TOPICAL (3+ sÅ‚Ã³w)
        word_count = data["word_count"]
        entity_type = "CONCEPT" if word_count <= 2 else "TOPICAL"
        
        # --- IMPORTANCE SCORE ---
        score = 0.0
        
        # 1. Distribution score (w ilu ÅºrÃ³dÅ‚ach, 0-0.35)
        #    Encja w 100% ÅºrÃ³deÅ‚ = 0.35
        distribution = sources_count / max(total_sources, 1)
        score += distribution * 0.35
        
        # 2. Frequency score (log scale, 0-0.25)
        freq_score = min(0.25, math.log(freq + 1) * 0.06)
        score += freq_score
        
        # 3. Specificity score (2-3 sÅ‚owa = najlepsze, 0-0.20)
        if word_count == 2:
            score += 0.20
        elif word_count == 3:
            score += 0.18
        elif word_count == 1:
            score += 0.10
        elif word_count >= 4:
            score += 0.08
        
        # 4. Keyword relevance boost (0-0.20)
        if keyword_words:
            chunk_words = set(most_common_form.split())
            overlap = chunk_words & keyword_words
            if overlap:
                relevance = len(overlap) / max(len(keyword_words), 1)
                score += relevance * 0.20
        
        # v51: Build per-source frequency list (include 0 for sources without this entity)
        per_src_counts = [data["freq_per_source"].get(i, 0) for i in range(total_sources)]
        
        entity = TopicalEntity(
            text=lemma_key,
            display_text=most_common_form,
            type=entity_type,
            frequency=freq,
            sources_count=sources_count,
            importance=min(1.0, score),
            contexts=data["contexts"],
            variants=variants,
            freq_per_source=per_src_counts,
        )
        entities.append(entity)
    
    # Sortuj po importance
    entities.sort(key=lambda x: x.importance, reverse=True)
    
    print(f"[TOPICAL] âœ… Found {len(entities)} concept entities "
          f"(returning top {min(max_entities, len(entities))})")
    
    return entities[:max_entities]


# ================================================================
# ğŸ“Š CONCEPT ENTITY SUMMARY
# ================================================================

def generate_topical_summary(
    entities: List[TopicalEntity],
    main_keyword: str = "",
) -> Dict[str, Any]:
    """
    Generuje podsumowanie topical entities dla promptÃ³w SEO.
    """
    if not entities:
        return {
            "status": "NO_DATA",
            "concepts": [],
            "agent_instruction": "",
        }
    
    # Top concepts (high importance, multi-source)
    must_cover = [e for e in entities if e.sources_count >= 2 and e.importance >= 0.3]
    should_cover = [e for e in entities if e not in must_cover and e.importance >= 0.2]
    
    # Instruction for the writing agent
    concept_list = ", ".join([e.display_text for e in must_cover[:10]])
    should_list = ", ".join([e.display_text for e in should_cover[:8]])
    
    instruction_parts = []
    
    if must_cover:
        instruction_parts.append(
            f"ğŸ¯ KLUCZOWE POJÄ˜CIA (wystÄ™pujÄ… u wiÄ™kszoÅ›ci konkurencji â€” MUSISZ je wpleÅ›Ä‡): "
            f"{concept_list}"
        )
    
    if should_cover:
        instruction_parts.append(
            f"ğŸ“Œ DODATKOWE POJÄ˜CIA (warto wspomnieÄ‡ dla peÅ‚nego pokrycia tematu): "
            f"{should_list}"
        )
    
    instruction_parts.append(
        "ğŸ’¡ UÅ¼yj tych pojÄ™Ä‡ naturalnie w tekÅ›cie â€” nie jako listy, "
        "ale wplecione w zdania. Google nagradza treÅ›ci, ktÃ³re pokrywajÄ… "
        "peÅ‚ne pole semantyczne tematu."
    )
    
    return {
        "status": "OK",
        "total_concepts": len(entities),
        "must_cover_count": len(must_cover),
        "should_cover_count": len(should_cover),
        "must_cover": [e.to_dict() for e in must_cover[:15]],
        "should_cover": [e.to_dict() for e in should_cover[:10]],
        "all_concepts": [e.display_text for e in entities[:30]],
        "agent_instruction": "\n".join(instruction_parts),
    }
